<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Whde"><link rel="alternative" href="/atom.xml" title="Whde" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Python爬取笑话 - Whde</title><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><script src="/js/jquery-3.1.1.min.js"></script><script src="/js/fancybox/jquery.fancybox.min.js"></script></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">Whde</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">目录</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2019-05-24T19:04:49.000Z">2019 - 05 - 25 03:04:49</time><h1 class="post__title"><a href="/2019/05/25/Joke/">Python爬取笑话</a></h1><div class="post__main echo"><h3 id="Python爬取笑话"><a href="#Python爬取笑话" class="headerlink" title="Python爬取笑话"></a>Python爬取笑话</h3><p>Python爬取笑话排行，将数据以json存储到文件中</p>
<p>我们爬取的地址是：<a href="http://www.jokeji.cn/hot.asp?action=brow" target="_blank" rel="noopener">http://www.jokeji.cn/hot.asp?action=brow</a></p>
<p>分析一下，我们需要爬取总页数，然后读取每页下面的笑话，笑话内容需要去详情页去爬取</p>
<h5 id="1、首先我们创建一个程序入口"><a href="#1、首先我们创建一个程序入口" class="headerlink" title="1、首先我们创建一个程序入口"></a>1、首先我们创建一个程序入口</h5><ul>
<li>我们创建一个文件夹，文件夹里最后存数据文件进去</li>
<li>spider(root_url) 方法开始爬取数据，这个方法我们自己实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 程序入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 创建文件夹，最后存数据到这个文件夹下面</span></span><br><span class="line">    importlib.reload(sys)</span><br><span class="line">    <span class="keyword">if</span> os.path.isdir(root_folder):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        os.mkdir(root_folder)</span><br><span class="line">    <span class="comment"># 开始爬取数据</span></span><br><span class="line">    spider(root_url)</span><br><span class="line">    print(<span class="string">'**** spider ****'</span>)</span><br></pre></td></tr></table></figure>

<h5 id="2、开始爬取数据spider-root-url"><a href="#2、开始爬取数据spider-root-url" class="headerlink" title="2、开始爬取数据spider(root_url)"></a>2、开始爬取数据spider(root_url)</h5><ul>
<li>先去getpages(url)获取页数pages，后面会讲到</li>
<li>page(pageurl)获取每页里面数据</li>
<li>数据爬完，存储到data.json文件中</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始爬取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spider</span><span class="params">(url)</span>:</span></span><br><span class="line">    list1 = []</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 去获取排行榜的页数</span></span><br><span class="line">    pages = getpages(url)</span><br><span class="line">    <span class="keyword">while</span> i &lt;= int(pages):</span><br><span class="line">        <span class="comment"># 拼接每一页的URL地址</span></span><br><span class="line">        pageurl = <span class="string">'http://www.jokeji.cn/hot.asp?action=brow&amp;me_page='</span>+str(i)</span><br><span class="line">        print(pageurl)</span><br><span class="line">        <span class="comment"># 获取每页下面的内容</span></span><br><span class="line">        list1 = list1+page(pageurl)</span><br><span class="line">        i = i+<span class="number">1</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'大于页数'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将list存储到data.json中</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        filename = root_folder + <span class="string">'data.json'</span></span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            stri = json.dumps(list1, encoding=<span class="string">'UTF-8'</span>, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">            print(stri)</span><br><span class="line">            f.write(stri)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'Error:'</span>, e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h5 id="3、获取页数getpages-url"><a href="#3、获取页数getpages-url" class="headerlink" title="3、获取页数getpages(url)"></a>3、获取页数getpages(url)</h5><p>找最后一页的逻辑(如图)，最后一页在这个<code>&gt;&gt;</code>按钮里面，我们看源码，可以看到598，598就是总的页数了，接下来就是怎么获取这个页数。我们找到Class=”main_title”，然后找这main_title下面的所有td，我们所要的598就在倒数第二个td中，最后处理字符串，取出598。</p>
<img src="/images/WX20181205-164111@2x.png" alt="最后一页的逻辑" width="90%" height="90%/">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取排行榜的页数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getpages</span><span class="params">(url)</span>:</span></span><br><span class="line">    print(url)</span><br><span class="line">    url = quote(url, safe=string.printable)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    http = urllib3.PoolManager()</span><br><span class="line">    web_data = http.request(<span class="string">'GET'</span>, url, headers=headers).data</span><br><span class="line">    <span class="comment"># web_data = requests.get(url, headers=headers)</span></span><br><span class="line">    <span class="comment"># web_data.encoding = 'gb2312'</span></span><br><span class="line">    soup = BeautifulSoup(web_data, <span class="string">'html.parser'</span>, from_encoding=<span class="string">'GBK'</span>)</span><br><span class="line">    print(web_data)</span><br><span class="line">    <span class="comment"># 找总页数</span></span><br><span class="line">    span = soup.find(class_=<span class="string">'main_title'</span>)</span><br><span class="line">    tds = span.findAll(<span class="string">'td'</span>)</span><br><span class="line">    td = tds[len(tds)<span class="number">-2</span>]</span><br><span class="line">    pages = td.select(<span class="string">'a'</span>)[<span class="number">0</span>].get(<span class="string">'href'</span>).replace(<span class="string">'hot.asp?action=brow&amp;me_page='</span>, <span class="string">''</span>)</span><br><span class="line">    print(pages)</span><br><span class="line">    <span class="keyword">return</span> pages</span><br></pre></td></tr></table></figure>

<h5 id="4、page-pageurl-获取每页里面数据"><a href="#4、page-pageurl-获取每页里面数据" class="headerlink" title="4、page(pageurl)获取每页里面数据"></a>4、page(pageurl)获取每页里面数据</h5><p>getmeichannel(url)获取当前页下面的笑话列表</p>
<p>然后再detail(herf)去爬取每个笑话的详情</p>
<p>爬取逻辑看图：</p>
<img src="/images/WX20181205-174145@2x.png" alt="WX20181205-174145@2x.png" width="90%" height="90%/">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取每页下面的内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment"># 获取每页下面的笑话list</span></span><br><span class="line">    channel_list = getmeichannel(url)</span><br><span class="line">    list1 = []</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> channel_list:</span><br><span class="line">        dict1 = &#123;&#125;</span><br><span class="line">        a = tr.find(class_=<span class="string">'main_14'</span>)</span><br><span class="line">        herf = <span class="string">'http://www.jokeji.cn'</span>+a.get(<span class="string">'href'</span>)</span><br><span class="line">        title = a.get_text()</span><br><span class="line">        print(str(herf)+<span class="string">' --- '</span>+str(title))</span><br><span class="line">        dict1[<span class="string">'herf'</span>] = herf</span><br><span class="line">        dict1[<span class="string">'title'</span>] = title</span><br><span class="line">        dict1[<span class="string">'date'</span>] = tr.find(class_=<span class="string">'date'</span>).get_text().replace(<span class="string">'\r\n          '</span>, <span class="string">''</span>)</span><br><span class="line">        <span class="comment"># 获取当前笑话的详情，去详情页爬取数据</span></span><br><span class="line">        dict1[<span class="string">'detail'</span>] = detail(herf)</span><br><span class="line">        list1.append(dict1)</span><br><span class="line">    <span class="keyword">return</span> list1</span><br></pre></td></tr></table></figure>

<h5 id="5、getmeichannel-url-获取当前页下面的笑话列表"><a href="#5、getmeichannel-url-获取当前页下面的笑话列表" class="headerlink" title="5、getmeichannel(url)获取当前页下面的笑话列表"></a>5、getmeichannel(url)获取当前页下面的笑话列表</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取排行榜URL页下面的笑话list数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getmeichannel</span><span class="params">(url)</span>:</span></span><br><span class="line">    url = quote(url, safe=string.printable)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    http = urllib3.PoolManager()</span><br><span class="line">    web_data = http.request(<span class="string">'GET'</span>, url, headers=headers).data</span><br><span class="line">    soup = BeautifulSoup(web_data, <span class="string">'html.parser'</span>, from_encoding=<span class="string">'GBK'</span>)</span><br><span class="line">    channel = []</span><br><span class="line">    tables = soup.findAll(height=<span class="string">'30'</span>)</span><br><span class="line">    <span class="keyword">for</span> table <span class="keyword">in</span> tables:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">for</span> tr <span class="keyword">in</span> table.findAll(<span class="string">'tr'</span>):</span><br><span class="line">                channel.append(tr)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">'Error:'</span>, e)</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> channel</span><br></pre></td></tr></table></figure>

<h5 id="6、获取详情数据detail-herf"><a href="#6、获取详情数据detail-herf" class="headerlink" title="6、获取详情数据detail(herf)"></a>6、获取详情数据detail(herf)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取详情页数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detail</span><span class="params">(url)</span>:</span></span><br><span class="line">    url = quote(url, safe=string.printable)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    http = urllib3.PoolManager()</span><br><span class="line">    r = http.request(<span class="string">'GET'</span>, url, headers=headers)</span><br><span class="line">    web_data = r.data</span><br><span class="line">    soup = BeautifulSoup(web_data, <span class="string">'html.parser'</span>, from_encoding=<span class="string">'GBK'</span>)</span><br><span class="line">    <span class="comment"># 查找详情页数据</span></span><br><span class="line">    font = soup.find(attrs=&#123;<span class="string">'id'</span>: <span class="string">'text110'</span>&#125;)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> font.get_text()</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(str(e))</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>源码地址<a href="https://github.com/whde/Joke.git" target="_blank" rel="noopener">https://github.com/whde/Joke.git</a></p>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/Mac/">Mac</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Python/">Python</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/笑话/">笑话</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/爬虫/">爬虫</a></li></ul></footer></article></main><footer class="foot"><div class="foot-copy">&copy; 2019 Whde</div></footer><script src="/js/scroller.js"></script><script src="/js/main.js"></script></body></html>